# ğŸ§  ViT-Implementation â€” Your Friendly Vision Transformer!

Welcome to **ViT-Implementation**, a from-scratch PyTorch build of the **Vision Transformer (ViT)** architecture! Inspired by Dosovitskiy et al.â€™s [ViT paper](https://arxiv.org/abs/2010.11929), this repo walks you through how to classify images using transformers â€” without relying on any pretrained models.

---

## ğŸš€ Quick Start

### ğŸ‹ï¸â€â™€ï¸ 1. Train the Model
```bash
python -m src.train
```

### ğŸ§  2. Evaluate the Model
```bash
python -m src.evaluate
```

### ğŸ“Š 3. Generate CSV with Results
```bash
python -m src.csv_generator
```

---

## ğŸ§© Features

- âœ… Patch Embeddings â€” split images into patches and process them like tokens.
- âœ… Multi-Head Self-Attention â€” multiple attention heads for rich feature extraction.
- âœ… Transformer Encoder Blocks â€” with residual connections and layer norm.
- âœ… GELU Activation â€” smoother than ReLU.
- âœ… Fully modular PyTorch code â€” clean, readable, and reusable.
- âœ… No pretrained shortcuts â€” everything built from scratch!

---

## ğŸ“¦ Requirements

Install dependencies with:
```bash
pip install -r requirements.txt
```

---

# ğŸ§  ViT-Implementation â€” Â¡Tu Transformer de VisiÃ³n Amigable!

Bienvenid@ a **ViT-Implementation**, una implementaciÃ³n desde cero del modelo **Vision Transformer (ViT)** en PyTorch. Inspirado en el [artÃ­culo original](https://arxiv.org/abs/2010.11929) de Dosovitskiy et al., este repositorio te guÃ­a paso a paso para clasificar imÃ¡genes usando transformadores â€” sin modelos preentrenados.

---

## ğŸš€ CÃ³mo Empezar

### ğŸ‹ï¸â€â™€ï¸ 1. Entrenar el Modelo
```bash
python -m src.train
```

### ğŸ§  2. Evaluar el Modelo
```bash
python -m src.evaluate
```

### ğŸ“Š 3. Generar CSV con Resultados
```bash
python -m src.csv_generator
```

---

## ğŸ§© CaracterÃ­sticas

- âœ… Patch Embeddings â€” divide imÃ¡genes en parches como tokens.
- âœ… Multi-Head Self-Attention â€” mÃºltiples cabezas de atenciÃ³n para captar mÃ¡s contexto.
- âœ… Bloques de Codificador Transformer â€” con conexiones residuales y normalizaciÃ³n.
- âœ… ActivaciÃ³n GELU â€” mÃ¡s suave que ReLU.
- âœ… CÃ³digo PyTorch modular â€” claro, legible y reutilizable.
- âœ… Todo construido desde cero â€” sin atajos preentrenados.

---

## ğŸ“¦ Requisitos

Instala las dependencias con:
```bash
pip install -r requirements.txt
```

---

