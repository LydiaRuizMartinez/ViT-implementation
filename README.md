
# ğŸ§  ViT-Implementation â€” Your Friendly Vision Transformer!

Welcome to **ViT-Implementation**, where image patches go to school, learn attention, and graduate as powerful classification wizards ğŸ“ğŸ“¸âœ¨

This repo contains a **from-scratch implementation** of the **Vision Transformer (ViT)** architecture using PyTorch. We've built everything ourselves: patch embeddings, attention heads, transformer blocks, and more â€” because why use pretrained models when you can **reinvent the wheel with style**? ğŸ˜

This implementation is inspired by the groundbreaking [Vision Transformer (ViT) paper](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al., which introduced the idea of applying transformer architectures directly to image classification.

---

## ğŸš€ Getting Started

To begin your journey with our ViT, follow these simple spells (commands):

### ğŸ‹ï¸â€â™€ï¸ 1. Train the model
```bash
python -m src.train
```

This will train your Vision Transformer on the Imagenette dataset (or whatever dataset you plug in). Make sure you stretch before running this â€” those attention heads are heavy!

---

### ğŸ§  2. Evaluate the model
```bash
python -m src.evaluate
```

This will evaluate `best_model.pt` and print out the accuracy of your model. Be ready to be impressed â€” or to go back and tune those hyperparameters (again ğŸ˜…).

---

### ğŸ“Š 3. Generate a CSV of your hyperparameters & results
```bash
python -m src.csv_generator
```

Want to keep track of your experimentation madness? This command will generate a neat CSV so you can compare models like a proper deep learning scientist. ğŸ§ªğŸ“ˆ

---

## ğŸ§© Whatâ€™s Inside?

Here's what makes this repo cool:

- âœ… **Patch Embeddings** â€” Slice up images into small pieces and feed them to the model like popcorn.
- âœ… **GELU Activation** â€” Not just ReLU... we're fancier than that.
- âœ… **Multi-Head Self-Attention** â€” Because one head isn't enough to understand all the image drama.
- âœ… **Transformer Encoder Blocks** â€” Each with residual connections and layer norms, obviously.
- âœ… **From-scratch Training Pipeline** â€” Built lovingly with PyTorch â¤ï¸
- âœ… **Modular & Readable Code** â€” So your future self doesnâ€™t hate you.

---

## ğŸ¤– Requirements

Make sure youâ€™ve got the basics installed:

Install dependencies with:
```bash
pip install -r requirements.txt
```

---

## ğŸ§™â€â™€ï¸ Fun Fact

This ViT is trained on images, but deep inside it's powered by **math, magic, and motivation**. Every line of code has been written to be understandable, reusable, and cool.

---

## ğŸ“ Credits

Made with â¤ï¸ by Lydia Ruiz MartÃ­nez.  
Inspired by the original [ViT paper](https://arxiv.org/abs/2010.11929) and a bit of âœ¨ stubbornness âœ¨ to implement things from scratch.

---

## ğŸŒŸ Star This Repo!

If you found this helpful or just had fun reading this README, consider giving it a â­ï¸. It fuels the Transformer (and the author)!
